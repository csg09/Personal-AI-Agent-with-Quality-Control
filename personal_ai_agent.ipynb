{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abaa4185",
   "metadata": {},
   "source": [
    "# Personal AI Agent with Quality Control\n",
    "\n",
    "Build a personal AI agent that represents you on your website or portfolio.\n",
    "This agent uses your LinkedIn profile and personal summary to answer questions\n",
    "about your background, skills, and experience with built-in quality control.\n",
    "\n",
    "## What You'll Learn\n",
    "- Reading and processing PDF files (LinkedIn profiles)\n",
    "- Building interactive UIs with Gradio\n",
    "- Implementing quality control with LLM evaluators\n",
    "- Creating self-correcting AI agents\n",
    "- Using structured outputs with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598b06f",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ddf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: Required Files and API Keys\n",
    "\n",
    "1. Create a .env file with your API key:\n",
    "   OPENAI_API_KEY=your_openai_key\n",
    "   GOOGLE_API_KEY=your_google_key (optional, for evaluator)\n",
    "\n",
    "2. Create a 'me' folder with:\n",
    "   - linkedin.pdf: Your LinkedIn profile as PDF\n",
    "   - summary.txt: A brief summary about yourself\n",
    "\n",
    "⚠️ Remember to add .env to your .gitignore!\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "from pydantic import BaseModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a7436",
   "metadata": {},
   "source": [
    "## Step 1: Load Your Personal Data\n",
    "\n",
    "Replace the files in the 'me' folder with your own:\n",
    "- Export your LinkedIn profile as PDF\n",
    "- Create a summary.txt with key highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LinkedIn PDF\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "print(\"LinkedIn profile loaded successfully!\")\n",
    "print(f\"Total characters: {len(linkedin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of the LinkedIn content\n",
    "print(\"Sample of LinkedIn content:\")\n",
    "print(\"=\" * 50)\n",
    "print(linkedin[:500] + \"...\" if len(linkedin) > 500 else linkedin)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read personal summary\n",
    "try:\n",
    "    with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read()\n",
    "    print(\"\\nSummary loaded successfully!\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Warning: summary.txt not found. Please create one in the 'me' folder.\")\n",
    "    summary = \"No summary available.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd9091",
   "metadata": {},
   "source": [
    "## Step 2: Configure Your Agent\n",
    "\n",
    "Update this with your actual name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea41cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your name!\n",
    "name = \"Your Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the system prompt for your personal agent\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3959d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Display the system prompt (first 500 characters)\n",
    "print(\"System Prompt Preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(system_prompt[:500] + \"...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fe9c5",
   "metadata": {},
   "source": [
    "## Step 3: Basic Chat Function\n",
    "\n",
    "Let's start with a simple chat interface before adding quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f837366",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat_basic(message, history):\n",
    "    \"\"\"\n",
    "    Basic chat function without quality control.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        history: Conversation history in Gradio format\n",
    "    \n",
    "    Returns:\n",
    "        Agent's response\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc2ff2",
   "metadata": {},
   "source": [
    "### Test the Basic Agent\n",
    "\n",
    "Uncomment the line below to launch the basic version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3373e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch basic version\n",
    "# gr.ChatInterface(chat_basic, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8986209",
   "metadata": {},
   "source": [
    "## Step 4: Add Quality Control System\n",
    "\n",
    "Now let's add an evaluator that checks the quality of responses\n",
    "before sending them to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1f28e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define structured output format for evaluation\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bee512",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create evaluator system prompt\n",
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd38156",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    \"\"\"\n",
    "    Creates the user prompt for the evaluator.\n",
    "    \n",
    "    Args:\n",
    "        reply: The agent's response to evaluate\n",
    "        message: The user's original message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt for evaluation\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0caecc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up evaluator (using Gemini, but you can use OpenAI too)\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if google_api_key:\n",
    "    gemini = OpenAI(\n",
    "        api_key=google_api_key, \n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    print(\"✓ Gemini evaluator configured\")\n",
    "else:\n",
    "    print(\"○ Google API key not found - will use OpenAI for evaluation\")\n",
    "    gemini = openai  # Fall back to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02ca49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Evaluates whether an agent's response is acceptable.\n",
    "    \n",
    "    Args:\n",
    "        reply: The agent's response to evaluate\n",
    "        message: The user's original message  \n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation object with is_acceptable (bool) and feedback (str)\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Try structured output (Gemini 2.0)\n",
    "        response = gemini.beta.chat.completions.parse(\n",
    "            model=\"gemini-2.0-flash-exp\", \n",
    "            messages=messages, \n",
    "            response_format=Evaluation\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except:\n",
    "        # Fallback for models without structured output\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        # Simple parsing\n",
    "        content = response.choices[0].message.content.lower()\n",
    "        is_acceptable = \"acceptable\" in content or \"yes\" in content\n",
    "        return Evaluation(is_acceptable=is_acceptable, feedback=response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d54c9d",
   "metadata": {},
   "source": [
    "### Test the Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7682c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "test_messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "test_response = \"I have extensive experience in AI and machine learning.\"\n",
    "\n",
    "test_evaluation = evaluate(test_response, \"What's your background?\", test_messages[:1])\n",
    "print(f\"Acceptable: {test_evaluation.is_acceptable}\")\n",
    "print(f\"Feedback: {test_evaluation.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b028e51",
   "metadata": {},
   "source": [
    "## Step 5: Add Self-Correction\n",
    "\n",
    "If the evaluator rejects a response, the agent will try again\n",
    "with feedback about what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e566c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    \"\"\"\n",
    "    Regenerates response with feedback from the evaluator.\n",
    "    \n",
    "    Args:\n",
    "        reply: The rejected response\n",
    "        message: User's original message\n",
    "        history: Conversation history\n",
    "        feedback: Evaluator's feedback on why it was rejected\n",
    "    \n",
    "    Returns:\n",
    "        New improved response\n",
    "    \"\"\"\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    updated_system_prompt += \"Please try again, addressing the feedback.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fcc26",
   "metadata": {},
   "source": [
    "## Step 6: Complete Chat Function with Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a3e41",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Main chat function with quality control and self-correction.\n",
    "    \n",
    "    Process:\n",
    "    1. Generate initial response\n",
    "    2. Evaluate response quality\n",
    "    3. If rejected, regenerate with feedback\n",
    "    4. Return final response\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        history: Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Agent's final response (after quality control)\n",
    "    \"\"\"\n",
    "    # Clean up history if needed (for non-OpenAI providers)\n",
    "    # Uncomment if you get errors with Groq or other providers:\n",
    "    # history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Generate initial response\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # Evaluate the response\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"✓ Passed evaluation - returning reply\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(\"✗ Failed evaluation - retrying\")\n",
    "        print(f\"Feedback: {evaluation.feedback}\")\n",
    "        # Regenerate with feedback\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "        print(\"✓ Generated improved response\")\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb8f72",
   "metadata": {},
   "source": [
    "## Step 7: Launch the Agent!\n",
    "\n",
    "Run this cell to start your personal AI agent with quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "gr.ChatInterface(\n",
    "    chat, \n",
    "    type=\"messages\",\n",
    "    title=f\"Chat with {name}'s AI Agent\",\n",
    "    description=f\"Ask me questions about {name}'s background, skills, and experience!\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6159bc",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Architecture:\n",
    "1. **User Question** → Agent generates response\n",
    "2. **Evaluator** → Checks if response is acceptable\n",
    "3. **Self-Correction** → If rejected, regenerates with feedback\n",
    "4. **Final Response** → Sent to user\n",
    "\n",
    "### Design Patterns Used:\n",
    "- **Reflection Pattern**: Agent evaluates its own output\n",
    "- **Self-Correction Pattern**: Agent improves based on feedback\n",
    "- **Quality Control**: Automated evaluation before user sees response\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Loads personal data from PDF and text files\n",
    "- ✅ Interactive chat interface with Gradio\n",
    "- ✅ Automated quality control\n",
    "- ✅ Self-correcting responses\n",
    "- ✅ Structured outputs with Pydantic\n",
    "- ✅ Professional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d69c9",
   "metadata": {},
   "source": [
    "## Extension Ideas\n",
    "\n",
    "1. **Multiple Evaluators**: Use different models to vote on quality\n",
    "2. **Evaluation Criteria**: Add specific checks (tone, accuracy, length)\n",
    "3. **Learning System**: Track which responses get rejected and why\n",
    "4. **Multi-Language**: Support questions in different languages\n",
    "5. **Voice Interface**: Add speech-to-text and text-to-speech\n",
    "6. **Analytics**: Track common questions and improve responses\n",
    "7. **A/B Testing**: Compare different system prompts\n",
    "8. **Context Memory**: Remember previous conversations\n",
    "9. **Source Citations**: Reference specific parts of LinkedIn/summary\n",
    "10. **Personality Tuning**: Adjust tone based on user feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77bbbc",
   "metadata": {},
   "source": [
    "## Tips for Better Results\n",
    "\n",
    "### LinkedIn PDF:\n",
    "- Export from LinkedIn: Profile → More → Save to PDF\n",
    "- Ensure text is selectable (not an image)\n",
    "- Include all relevant sections\n",
    "\n",
    "### Summary.txt:\n",
    "- Highlight unique achievements\n",
    "- Include specific technologies/skills\n",
    "- Mention notable projects\n",
    "- Keep it concise (200-500 words)\n",
    "\n",
    "### System Prompt:\n",
    "- Be specific about tone (professional, friendly, technical)\n",
    "- Include examples of good responses\n",
    "- Set clear boundaries (what not to discuss)\n",
    "- Customize for your audience\n",
    "\n",
    "### Evaluator:\n",
    "- Define specific quality criteria\n",
    "- Include examples of good/bad responses\n",
    "- Adjust strictness based on use case\n",
    "- Consider multiple evaluation rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69c88d",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### \"FileNotFoundError: me/linkedin.pdf\"\n",
    "- Create a 'me' folder in the same directory\n",
    "- Add your linkedin.pdf file\n",
    "- Or update the path in the code\n",
    "\n",
    "### \"Gradio connection error\"\n",
    "- Check if port 7860 is available\n",
    "- Try: `gr.ChatInterface(...).launch(server_port=7861)`\n",
    "\n",
    "### \"Evaluator always accepts/rejects\"\n",
    "- Review evaluator_system_prompt\n",
    "- Add more specific criteria\n",
    "- Test with different questions\n",
    "\n",
    "### \"History format error\" (Groq, etc.)\n",
    "- Uncomment the history cleaning line in chat()\n",
    "- Some providers are strict about message format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76a8e6",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Privacy**: All processing happens via API calls\n",
    "- **Costs**: Uses gpt-4o-mini (very affordable)\n",
    "- **Performance**: Usually responds in 2-5 seconds\n",
    "- **Quality**: Self-correction significantly improves responses\n",
    "- **Customization**: Easily adaptable for different use cases"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
